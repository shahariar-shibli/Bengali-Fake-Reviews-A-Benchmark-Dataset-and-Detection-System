{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD_CGqiUxf_o"
      },
      "outputs": [],
      "source": [
        "pip install neattext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAAveYt4QQr0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import neattext.functions as nfx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, LSTM, Dropout, Conv1D, GlobalMaxPooling1D, MaxPooling1D,  Activation\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mup-gChDE-qN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import read_excel\n",
        "import numpy as np\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "#from unidecode import unidecode\n",
        "import os\n",
        "from time import time \n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling2D, Conv1D, MaxPooling1D\n",
        "#from keras.optimizers import SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import h5py\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcX_TtZnPWsz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJKoRX35QcLh"
      },
      "outputs": [],
      "source": [
        "pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6L71mKiIL5g"
      },
      "outputs": [],
      "source": [
        "traind = pd.read_excel(\"/content/nlpaug_4augmented_train.xlsx\")\n",
        "vald = pd.read_excel(\"/content/nlpaug_4augmented_val.xlsx\")\n",
        "testd = pd.read_excel(\"/content/nlpaug_4augmented_test.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soFpc8QlQcLi"
      },
      "outputs": [],
      "source": [
        "traind[\"Review\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ru1KP7J_wlq"
      },
      "outputs": [],
      "source": [
        "train_I=traind[\"Review\"].to_numpy()\n",
        "val_I=vald[\"Review\"].to_numpy()\n",
        "test_I=testd[\"Review\"].to_numpy()\n",
        "train_L=traind[\"Label\"].to_numpy()\n",
        "val_L=vald[\"Label\"].to_numpy()\n",
        "test_L=testd[\"Label\"].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsB-E5DgApyq"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.utils.np_utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF-QIdF0AYJe"
      },
      "outputs": [],
      "source": [
        "training_sentences = []\n",
        "testing_sentences = []\n",
        "val_sentences = []\n",
        "train_sentences = list(train_I)\n",
        "train_labels = train_L\n",
        "for i in range(len(train_sentences)): \n",
        "    #print(train_sentences[i])\n",
        "    x=str(train_sentences[i])\n",
        "    training_sentences.append(x)\n",
        "training_sentences=np.array(training_sentences)\n",
        "\n",
        "test_sentences=list(test_I)\n",
        "test_labels=test_L\n",
        "\n",
        "for i in range(len(test_sentences)): \n",
        "    x=str(test_sentences[i])\n",
        "    testing_sentences.append(x)\n",
        "testing_sentences=np.array(testing_sentences)\n",
        "\n",
        "validation_sentences=list(val_I)\n",
        "val_labels=val_L\n",
        "\n",
        "for i in range(len(validation_sentences)): \n",
        "    x=str(validation_sentences[i])\n",
        "    val_sentences.append(x)\n",
        "val_sentences=np.array(val_sentences)\n",
        "\n",
        "\n",
        "train_labels=to_categorical(train_labels)\n",
        "test_labels=to_categorical(test_labels)\n",
        "val_labels=to_categorical(val_labels)\n",
        "\n",
        "# print(\"Training Set Length: \"+str(len(train1)))\n",
        "# print(\"Testing Set Length: \"+str(len(test1)))\n",
        "print(\"training_sentences shape: \"+str(training_sentences.shape))\n",
        "print(\"testing_sentences shape: \"+str(testing_sentences.shape))\n",
        "print(\"validation_sentences shape: \"+str(val_sentences.shape))\n",
        "print(\"train_labels shape: \"+str(train_labels.shape))\n",
        "print(\"test_labels shape: \"+str(test_labels.shape))\n",
        "print(\"val_labels shape: \"+str(val_labels.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIRu9pb0DUB4"
      },
      "outputs": [],
      "source": [
        "vocab_size = 25000\n",
        "embedding_dim = 300\n",
        "max_length = 512\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbnT5DWUD1ge"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "print(\"Word index length:\"+str(len(tokenizer.word_index)))\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(test_sequences,maxlen=max_length)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "val_padded = pad_sequences(val_sequences,maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSJmazeOEhpc"
      },
      "outputs": [],
      "source": [
        "print(\"Sentence :--> \\n\")\n",
        "print(training_sentences[2]+\"\\n\")\n",
        "print(\"Sentence Tokenized and Converted into Sequence :--> \\n\")\n",
        "print(str(sequences[2])+\"\\n\")\n",
        "print(\"After Padding the Sequence with padding length 100 :--> \\n\")\n",
        "print(padded[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYUyCqjOmRUK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    import keras\n",
        "    from keras.layers import Conv1D,MaxPooling1D,Flatten,Dense,Dropout,Embedding\n",
        "    max_words_len=350\n",
        "    encoder_input=keras.Input(shape=(max_words_len,),)\n",
        "    encoder_embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
        "    encoder_embedding_layer=encoder_embedding(encoder_input)\n",
        "    conv1D_1=Conv1D(512, 4, activation='relu',name='con1')(encoder_embedding_layer)\n",
        "    maxPool1D_1=MaxPooling1D(pool_size=2,name='maxpool1')(conv1D_1)\n",
        "    conv1D_2=Conv1D(256, 3, activation='relu',name='con2')(maxPool1D_1)\n",
        "    maxPool1D_2=MaxPooling1D(pool_size=2,name='maxpool2')(conv1D_2)\n",
        "    conv1D_3=Conv1D(128, 2, activation='relu',name='con3')(maxPool1D_2)\n",
        "    maxPool1D_3=MaxPooling1D(pool_size=2,name='maxpool3')(conv1D_3)\n",
        "    # Apply Bidirectional LSTM over embedded inputs\n",
        "    lstm_outs = tf.keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(100, return_sequences=True)\n",
        "    )(maxPool1D_3)\n",
        "\n",
        "    # Attention Mechanism - Generate attention vectors\n",
        "    input_dim = int(lstm_outs.shape[2])\n",
        "    permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n",
        "    attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n",
        "    print(attention_vector.shape)\n",
        "    attention_vector = keras.layers.Reshape((attention_vector.shape[1],))(attention_vector)\n",
        "    attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n",
        "    attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n",
        "    fc = keras.layers.Dense(100, activation='relu')(attention_output)\n",
        "    output = keras.layers.Dense(2, activation='softmax')(fc)\n",
        "    model = keras.Model(inputs=[encoder_input], outputs=output)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Mo5k69QcLk"
      },
      "outputs": [],
      "source": [
        "epoch=15\n",
        "batch=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O_Kvqg1mVWC"
      },
      "outputs": [],
      "source": [
        "history=model.fit(padded,train_labels,epochs=epoch,batch_size=batch,validation_data=(val_padded,val_labels),use_multiprocessing=True, workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nu0DOMLFl6C"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()\n",
        "\n",
        "accuracy = history.history['accuracy']\n",
        "val_accuracy= history.history['val_accuracy']\n",
        "plt.plot(accuracy)\n",
        "plt.plot(val_accuracy)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaedzP0mFqq7"
      },
      "outputs": [],
      "source": [
        "predict_x=model.predict(testing_padded) \n",
        "classes_xy=np.argmax(predict_x,axis=1)\n",
        "loss_and_metrics = model.evaluate(testing_padded,test_labels,batch_size=batch)\n",
        "print(\"The test accuracy is: \"+str(loss_and_metrics[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiNUmiIrQcLl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "x=roc_auc_score(test_labels,model.predict(testing_padded) )\n",
        "print(x)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "y_true=np.argmax(test_labels,axis=1)\n",
        "classes_x=[-1 if item==0 else item for item in classes_xy]\n",
        "y_true=[-1 if item==0 else item for item in y_true]\n",
        "mcc=matthews_corrcoef(y_true,classes_x)\n",
        "print(mcc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf=confusion_matrix(y_true, classes_x)\n",
        "print(conf[0][0])\n",
        "print(conf[0][1])\n",
        "print(conf[1][0])\n",
        "print(conf[1][1])\n",
        "cr=classification_report(np.argmax(test_labels,axis=1), classes_xy,digits=4,output_dict=True)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRcevWR-QcLl"
      },
      "outputs": [],
      "source": [
        "training_stats=[]\n",
        "training_stats.append(\n",
        "        {\n",
        "            'roc-auc': x,\n",
        "            'mcc': mcc,\n",
        "            'conf_0_0': conf[0][0],\n",
        "            'conf_0_1': conf[0][1],\n",
        "            'conf_1_0': conf[1][0],\n",
        "            'conf_1_1': conf[1][1],\n",
        "            'cr': cr\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTfj4zosQcLl"
      },
      "outputs": [],
      "source": [
        "id=4\n",
        "modelname=\"CNNBiLSTM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ_5CdpmQcLl"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(training_stats) \n",
        "df2 = pd.DataFrame(history.history) \n",
        "df3 = pd.DataFrame(predict_x)\n",
        "df4 = pd.DataFrame(classes_xy,columns=['Pred'])\n",
        "df4['True']=np.argmax(test_labels,axis=1)\n",
        "df5= pd.DataFrame(cr).transpose()\n",
        "    \n",
        "# saving the dataframe \n",
        "df.to_csv(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/performance\"+str(id)+str(modelname)+\".csv\") \n",
        "df2.to_csv(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/history\"+str(id)+str(modelname)+\".csv\") \n",
        "df3.to_csv(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/probab\"+str(id)+str(modelname)+\".csv\") \n",
        "df4.to_csv(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/predict\"+str(id)+str(modelname)+\".csv\") \n",
        "df5.to_csv(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/cr\"+str(id)+str(modelname)+\".csv\") \n",
        "model.save(\"C:/Users/MSI/Desktop/BFRD/Final/nlpaug/model\"+str(id)+str(modelname)+\".h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNcaxJdJSh2n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}